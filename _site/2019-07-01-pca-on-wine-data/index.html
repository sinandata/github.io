<!DOCTYPE html>
<html lang="en">
  <!-- Beautiful Jekyll | MIT license | Copyright Dean Attali 2016 -->
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, viewport-fit=cover">

  <title>PCA on Wine Quality Dataset</title>

  <meta name="author" content="Sinan Gok" />

  
  <meta name="description" content="Principal Component Analysis for unsupervised learning">
  

  <link rel="alternate" type="application/rss+xml" title="Sinan Gok, PhD - My data science portfolio" href="http://localhost:4000/feed.xml" />

  

  

  


  
    
      
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.0/css/font-awesome.min.css" />


    
  

  
    
      <link rel="stylesheet" href="/css/bootstrap.min.css" />
    
      <link rel="stylesheet" href="/css/bootstrap-social.css" />
    
      <link rel="stylesheet" href="/css/main.css" />
    
  

  
    
      <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" />
    
      <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" />
    
  

  

  

  

    <!-- Facebook OpenGraph tags -->
  

  
  <meta property="og:title" content="PCA on Wine Quality Dataset" />
  

   
  <meta property="og:description" content="Principal Component Analysis for unsupervised learning">
  


  <meta property="og:type" content="website" />

  
  <meta property="og:url" content="http://localhost:4000/2019-07-01-pca-on-wine-data/" />
  <link rel="canonical" href="http://localhost:4000/2019-07-01-pca-on-wine-data/" />
  

  
  <meta property="og:image" content="http://localhost:4000/img/sakal-profile.jpeg" />
  


  <!-- Twitter summary cards -->
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:site" content="@" />
  <meta name="twitter:creator" content="@" />

  
  <meta name="twitter:title" content="PCA on Wine Quality Dataset" />
  

  
  <meta name="twitter:description" content="Principal Component Analysis for unsupervised learning">
  

  
  <meta name="twitter:image" content="http://localhost:4000/img/sakal-profile.jpeg" />
  

  

  

</head>


  <body>

    

  
    <nav class="navbar navbar-default navbar-fixed-top navbar-custom">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button><a class="navbar-brand" href="http://localhost:4000/">Sinan Gok, PhD</a></div>

    <div class="collapse navbar-collapse" id="main-navbar">
      <ul class="nav navbar-nav navbar-right">
          <li><a href="/aboutme">About Me</a></li>
          <li class="navlinks-container">
            <a class="navlinks-parent" href="javascript:void(0)">Showcase</a>
            <div class="navlinks-children">
              <a href="/mlpipeline">ML Pipeline</a>
            </div>
          </li>
        
          <li class="navlinks-container">
            <a class="navlinks-parent" href="javascript:void(0)">Articles</a>
            <div class="navlinks-children">
              <a href="https://medium.com/@goksinan/object-oriented-programming-oop-in-python-a-short-introduction-cd32b4c84393">OOP in Python</a>
              <a href="https://medium.com/@goksinan/demystifying-the-name-check-in-python-dce439472a83">The __name__ check</a>
            </div>
          </li>
        
          <li><a href="/resources">Resources</a></li></ul>
    </div>

	
	<div class="avatar-container">
	  <div class="avatar-img-border">
	    <a href="http://localhost:4000/">
	      <img class="avatar-img" src="/img/sakal-profile.jpeg" />
		</a>
	  </div>
	</div>
	

  </div>
</nav>


    <!-- TODO this file has become a mess, refactor it -->





<header class="header-section ">

<div class="intro-header no-img">
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <div class="post-heading">
          <h1>PCA on Wine Quality Dataset</h1>
		  
		    
			<h2 class="post-subheading">Principal Component Analysis for unsupervised learning</h2>
			
		  
		  
		  
		  <span class="post-meta">Posted on July 1, 2019</span>
		  
        </div>
      </div>
    </div>
  </div>
</div>
</header>





<div class="container">
  <div class="row">
    <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">

      

      <article role="main" class="blog-post">
        <p>We will use the Wine Quality Data Set for red wines created by P. Cortez et al. It has 11 variables and 1600 observations.</p>

<p><strong>Data science problem:</strong> Find out which features of wine are important to determine its quality.</p>

<p>We will use the Wine Quality Data Set for red wines created by P. Cortez et al. It has 11 variables and 1600 observations.</p>

<p>Steps to be taken from a <em>data science</em> perspective:</p>

<ol>
  <li><a href="#1-research-goal">Set the research goal:</a> We want to explain what properties of wine define the quality.</li>
  <li><a href="#2-acquire-data">Acquire data:</a> We will download the data set from a repository.</li>
  <li><a href="#3-prepare-data">Prepare data:</a> We will prepare data for the analysis.</li>
  <li><a href="#4-model-selection">Build model:</a> Build machine learning model you want to use for data analysis.
    <ul>
      <li><a href="#a-initial-inspection">Initial inspection</a></li>
      <li><a href="#b-interpreting-the-results">Interpreting the results</a></li>
      <li><a href="#c-predictive-model">Predictive model</a></li>
    </ul>
  </li>
</ol>

<ul>
  <li><a href="#resources">Resources</a></li>
  <li><a href="#link-to-github-repo">Link to GitHub repo</a></li>
</ul>

<h2 id="1-research-goal">1. Research goal</h2>

<p>We want to investigate what properties of wine define its quality.</p>

<p>We also want to see which of those properties are required to predict the quality of wine. This way we can safely ignore the unnecessary information next time we collect new data.</p>

<h2 id="2-acquire-data">2. Acquire data</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="n">url</span> <span class="o">=</span> <span class="s">'http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s">";"</span><span class="p">)</span>
</code></pre></div></div>

<p>Let’s see the first few rows of the dataset:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fixed acidity</th>
      <th>volatile acidity</th>
      <th>citric acid</th>
      <th>residual sugar</th>
      <th>chlorides</th>
      <th>free sulfur dioxide</th>
      <th>total sulfur dioxide</th>
      <th>density</th>
      <th>pH</th>
      <th>sulphates</th>
      <th>alcohol</th>
      <th>quality</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>7.4</td>
      <td>0.70</td>
      <td>0.00</td>
      <td>1.9</td>
      <td>0.076</td>
      <td>11.0</td>
      <td>34.0</td>
      <td>0.9978</td>
      <td>3.51</td>
      <td>0.56</td>
      <td>9.4</td>
      <td>5</td>
    </tr>
    <tr>
      <th>1</th>
      <td>7.8</td>
      <td>0.88</td>
      <td>0.00</td>
      <td>2.6</td>
      <td>0.098</td>
      <td>25.0</td>
      <td>67.0</td>
      <td>0.9968</td>
      <td>3.20</td>
      <td>0.68</td>
      <td>9.8</td>
      <td>5</td>
    </tr>
    <tr>
      <th>2</th>
      <td>7.8</td>
      <td>0.76</td>
      <td>0.04</td>
      <td>2.3</td>
      <td>0.092</td>
      <td>15.0</td>
      <td>54.0</td>
      <td>0.9970</td>
      <td>3.26</td>
      <td>0.65</td>
      <td>9.8</td>
      <td>5</td>
    </tr>
    <tr>
      <th>3</th>
      <td>11.2</td>
      <td>0.28</td>
      <td>0.56</td>
      <td>1.9</td>
      <td>0.075</td>
      <td>17.0</td>
      <td>60.0</td>
      <td>0.9980</td>
      <td>3.16</td>
      <td>0.58</td>
      <td>9.8</td>
      <td>6</td>
    </tr>
    <tr>
      <th>4</th>
      <td>7.4</td>
      <td>0.70</td>
      <td>0.00</td>
      <td>1.9</td>
      <td>0.076</td>
      <td>11.0</td>
      <td>34.0</td>
      <td>0.9978</td>
      <td>3.51</td>
      <td>0.56</td>
      <td>9.4</td>
      <td>5</td>
    </tr>
  </tbody>
</table>
</div>

<h2 id="3-prepare-data">3. Prepare data</h2>

<p>The last column is the target variable. We will use the rest as predictor variables. Slice from first column to one before the last.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="s">'fixed acidity'</span><span class="p">:</span><span class="s">'alcohol'</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">'quality'</span><span class="p">]</span>
</code></pre></div></div>

<p>Double check:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span><span class="o">.</span><span class="n">columns</span> <span class="c"># X is a pandas data frame</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Index(['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',
       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',
       'pH', 'sulphates', 'alcohol'],
      dtype='object')
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span><span class="o">.</span><span class="n">name</span> <span class="c"># y is a pandas series</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>'quality'
</code></pre></div></div>

<p>Before performing PCA, the dataset has to be standardized (i.e. subtracting mean, dividing by the standard deviation) The scikit-learn PCA package probably performs this internally, but we will do it anyway.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">scaler</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>StandardScaler(copy=True, with_mean=True, with_std=True)
</code></pre></div></div>

<p>We can see the mean and STD of each variable used for standardization:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">'Mean of each variable:'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">mean_</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">Std of each variable:'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">scale_</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Mean of each variable:
[ 8.31963727  0.52782051  0.27097561  2.5388055   0.08746654 15.87492183
 46.46779237  0.99674668  3.3111132   0.65814884 10.42298311]

Std of each variable:
[1.74055180e+00 1.79003704e-01 1.94740214e-01 1.40948711e+00
 4.70505826e-02 1.04568856e+01 3.28850367e+01 1.88674370e-03
 1.54338181e-01 1.69453967e-01 1.06533430e+00]
</code></pre></div></div>

<p>Perform transformation:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="4-model-selection">4. Model selection</h2>

<h3 id="a-initial-inspection">a. Initial inspection</h3>

<p>We want to use PCA and take a closer look at the latent variables.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span> <span class="c"># creates an instance of PCA class</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c"># applies PCA on predictor variables</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c"># create a new array of latent variables</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="n">sns</span><span class="o">.</span><span class="nb">set</span><span class="p">()</span>


<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span> <span class="c"># scree plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/images/pca-wine/scree-plot.png" alt="png" /></p>

<p>The above is called a <strong>scree plot</strong>. It shows the variances explained by each latent variable. The first component explains approx. 28% of the variance in the whole dataset.</p>

<p>Ideally, we would like to see an <strong>elbow shape</strong> in order to decide which PCs to keep and which ones to disregard. In practice, this rarely happens. Most of the time, we use enough PCs so that they explain 95% or 99% of the variation in the data.</p>

<p>By examining the above figure, we can conclude that first 6 variables contain most of the information inside the data.</p>

<h3 id="b-interpreting-the-results">b. Interpreting the results</h3>

<p>Once we apply the PCA, we are no longer in our familiar domain. We are in a different domain in which the latents are the linear combinations of the original variables, but they don’t represent any meaningful properties. Thus, it is impossible to interpret them by themselves.</p>

<p>We usually look at the correlation between the latent variable and original variables. If any of the original variables correlate well with the first few PCs, we usually conclude that the PCs are mainly influenced by the said variables, thus they must be the important ones.</p>

<p>The other approach is that we look at the PCA coefficients. These coefficients tell us how much of the original variables are used in creating the PCs. The higher the coefficient, the more important is the related variable.</p>

<p>Let’s put the component (PCA coefficients) into a data frame to see more comfortably:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">components_</span><span class="p">)</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.489314</td>
      <td>-0.238584</td>
      <td>0.463632</td>
      <td>0.146107</td>
      <td>0.212247</td>
      <td>-0.036158</td>
      <td>0.023575</td>
      <td>0.395353</td>
      <td>-0.438520</td>
      <td>0.242921</td>
      <td>-0.113232</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.110503</td>
      <td>0.274930</td>
      <td>-0.151791</td>
      <td>0.272080</td>
      <td>0.148052</td>
      <td>0.513567</td>
      <td>0.569487</td>
      <td>0.233575</td>
      <td>0.006711</td>
      <td>-0.037554</td>
      <td>-0.386181</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-0.123302</td>
      <td>-0.449963</td>
      <td>0.238247</td>
      <td>0.101283</td>
      <td>-0.092614</td>
      <td>0.428793</td>
      <td>0.322415</td>
      <td>-0.338871</td>
      <td>0.057697</td>
      <td>0.279786</td>
      <td>0.471673</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-0.229617</td>
      <td>0.078960</td>
      <td>-0.079418</td>
      <td>-0.372793</td>
      <td>0.666195</td>
      <td>-0.043538</td>
      <td>-0.034577</td>
      <td>-0.174500</td>
      <td>-0.003788</td>
      <td>0.550872</td>
      <td>-0.122181</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-0.082614</td>
      <td>0.218735</td>
      <td>-0.058573</td>
      <td>0.732144</td>
      <td>0.246501</td>
      <td>-0.159152</td>
      <td>-0.222465</td>
      <td>0.157077</td>
      <td>0.267530</td>
      <td>0.225962</td>
      <td>0.350681</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.101479</td>
      <td>0.411449</td>
      <td>0.069593</td>
      <td>0.049156</td>
      <td>0.304339</td>
      <td>-0.014000</td>
      <td>0.136308</td>
      <td>-0.391152</td>
      <td>-0.522116</td>
      <td>-0.381263</td>
      <td>0.361645</td>
    </tr>
    <tr>
      <th>6</th>
      <td>-0.350227</td>
      <td>-0.533735</td>
      <td>0.105497</td>
      <td>0.290663</td>
      <td>0.370413</td>
      <td>-0.116596</td>
      <td>-0.093662</td>
      <td>-0.170481</td>
      <td>-0.025138</td>
      <td>-0.447469</td>
      <td>-0.327651</td>
    </tr>
    <tr>
      <th>7</th>
      <td>-0.177595</td>
      <td>-0.078775</td>
      <td>-0.377516</td>
      <td>0.299845</td>
      <td>-0.357009</td>
      <td>-0.204781</td>
      <td>0.019036</td>
      <td>-0.239223</td>
      <td>-0.561391</td>
      <td>0.374604</td>
      <td>-0.217626</td>
    </tr>
    <tr>
      <th>8</th>
      <td>-0.194021</td>
      <td>0.129110</td>
      <td>0.381450</td>
      <td>-0.007523</td>
      <td>-0.111339</td>
      <td>-0.635405</td>
      <td>0.592116</td>
      <td>-0.020719</td>
      <td>0.167746</td>
      <td>0.058367</td>
      <td>-0.037603</td>
    </tr>
    <tr>
      <th>9</th>
      <td>-0.249523</td>
      <td>0.365925</td>
      <td>0.621677</td>
      <td>0.092872</td>
      <td>-0.217671</td>
      <td>0.248483</td>
      <td>-0.370750</td>
      <td>-0.239990</td>
      <td>-0.010970</td>
      <td>0.112320</td>
      <td>-0.303015</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.639691</td>
      <td>0.002389</td>
      <td>-0.070910</td>
      <td>0.184030</td>
      <td>0.053065</td>
      <td>-0.051421</td>
      <td>0.068702</td>
      <td>-0.567332</td>
      <td>0.340711</td>
      <td>0.069555</td>
      <td>-0.314526</td>
    </tr>
  </tbody>
</table>
</div>

<p>The above is the <em>coefficient matrix</em>. The first row is the coefficients that generated the first PC. In other words, the first PC was generated using the following formula:</p>

<p>PC1 = (fixed acidity * 0.489314) + (volatile acidity * -0.238584) + … + (alcohol * -0.113232)</p>

<p>If we choose to use the first 6 PCs, what do we call them? This is a bit tricky, because they are wieghted combinations of the original variables. Thus, a wine expert may help us to name them. Let’s assume that the expert gave us the following names for the new (latent) variables: 1.Acidity, 2.Sulfides, 3.More alcohol, 4.Chlorides, 5.More residual sugar, 6. Less pH</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">Z</span><span class="p">[:,:</span><span class="mi">6</span><span class="p">],</span> <span class="n">columns</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span>
<span class="p">[</span><span class="s">u'Acidity'</span><span class="p">,</span> <span class="s">u'Sulfides'</span><span class="p">,</span> <span class="s">u'More alcohol'</span><span class="p">,</span> <span class="s">u'Chlorides'</span><span class="p">,</span> <span class="s">u'More residual sugar'</span><span class="p">,</span> <span class="s">u'Less pH'</span><span class="p">]))</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Acidity</th>
      <th>Sulfides</th>
      <th>More alcohol</th>
      <th>Chlorides</th>
      <th>More residual sugar</th>
      <th>Less pH</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-1.619530</td>
      <td>0.450950</td>
      <td>-1.774454</td>
      <td>0.043740</td>
      <td>0.067014</td>
      <td>-0.913921</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.799170</td>
      <td>1.856553</td>
      <td>-0.911690</td>
      <td>0.548066</td>
      <td>-0.018392</td>
      <td>0.929714</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-0.748479</td>
      <td>0.882039</td>
      <td>-1.171394</td>
      <td>0.411021</td>
      <td>-0.043531</td>
      <td>0.401473</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2.357673</td>
      <td>-0.269976</td>
      <td>0.243489</td>
      <td>-0.928450</td>
      <td>-1.499149</td>
      <td>-0.131017</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-1.619530</td>
      <td>0.450950</td>
      <td>-1.774454</td>
      <td>0.043740</td>
      <td>0.067014</td>
      <td>-0.913921</td>
    </tr>
    <tr>
      <th>5</th>
      <td>-1.583707</td>
      <td>0.569195</td>
      <td>-1.538286</td>
      <td>0.023750</td>
      <td>-0.110076</td>
      <td>-0.993626</td>
    </tr>
    <tr>
      <th>6</th>
      <td>-1.101464</td>
      <td>0.608015</td>
      <td>-1.075915</td>
      <td>-0.343959</td>
      <td>-1.133382</td>
      <td>0.175000</td>
    </tr>
    <tr>
      <th>7</th>
      <td>-2.248708</td>
      <td>-0.416835</td>
      <td>-0.986837</td>
      <td>-0.001203</td>
      <td>-0.780435</td>
      <td>0.286057</td>
    </tr>
    <tr>
      <th>8</th>
      <td>-1.086887</td>
      <td>-0.308569</td>
      <td>-1.518150</td>
      <td>0.003315</td>
      <td>-0.226727</td>
      <td>-0.512634</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.654790</td>
      <td>1.665207</td>
      <td>1.209476</td>
      <td>-0.824635</td>
      <td>1.718501</td>
      <td>-0.476497</td>
    </tr>
  </tbody>
</table>
</div>

<p><strong>Usually, naming the new variables is dangerous unless we confidently conclude that the latent variables represent known properties. This requires expert knowledge.</strong></p>

<h3 id="c-predictive-model">c. Predictive model</h3>

<p>We can compare the prediction powers of the original variables and latent variables. Since the target is a categorical variable, we will perform a classification operation. We will use KNN for this purpose.</p>

<p><strong>1. Using the original data set</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">accuracy_score</span>

<span class="n">neigh</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">neigh</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">pred</span> <span class="o">=</span> <span class="n">neigh</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Confusion matrix:'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">y</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">Accuracy:'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">y</span><span class="p">))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Confusion matrix:
[[  6   5   2   0   0   0]
 [  0  18  10  13   0   0]
 [  2  17 589  87  18   2]
 [  2  12  75 511  36   7]
 [  0   1   4  27 144   7]
 [  0   0   1   0   1   2]]

Accuracy:
0.7942464040025016
</code></pre></div></div>

<p><strong>2. Using the first 6 PCs</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">neigh</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">neigh</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Z</span><span class="p">[:,:</span><span class="mi">6</span><span class="p">],</span><span class="n">y</span><span class="p">)</span>
<span class="n">pred</span> <span class="o">=</span> <span class="n">neigh</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Z</span><span class="p">[:,:</span><span class="mi">6</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Confusion matrix:'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">y</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">Accuracy:'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">y</span><span class="p">))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Confusion matrix:
[[  7   4   5   1   0   0]
 [  0  18  11  11   3   0]
 [  2  16 584  92  26   4]
 [  1  14  75 507  37   6]
 [  0   1   6  27 132   5]
 [  0   0   0   0   1   3]]

Accuracy:
0.7823639774859287
</code></pre></div></div>

<p>Using 6 variables instead of 11, we achive almost the same accuracy in our prediction.</p>

<p>Note: Here, we used the training set for prediction. Ideally, we would want to use two separate sets for training the model and testing it.</p>

<h2 id="resources">Resources</h2>

<ul>
  <li>D. Cielen, A. Meysman, M. Ali, Introducing Data Science: Big Data Machine Learning and more using Python tools, Manning Publications, 2016.</li>
  <li>Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.</li>
  <li>P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.</li>
</ul>

<h2 id="link-to-github-repo">Link to GitHub repo</h2>

<p>You can download the Notebook from my <a href="https://github.com/goksinan/pca_on_wine_quality_data.git">GitHub page</a></p>

      </article>

      
        <div class="blog-tags">
          Tags:
          
          
            <a href="/tags#Unsupervised learning">Unsupervised learning</a>
          
            <a href="/tags#PCA">PCA</a>
          
          
        </div>
      

      
        <!-- Check if any share-links are active -->




<section id = "social-share-section">
  <span class="sr-only">Share: </span>

  
  <!--- Share on Twitter -->
    <a href="https://twitter.com/intent/tweet?text=PCA+on+Wine+Quality+Dataset&url=http%3A%2F%2Flocalhost%3A4000%2F2019-07-01-pca-on-wine-data%2F"
      class="btn btn-social-icon btn-twitter" title="Share on Twitter">
      <span class="fa fa-fw fa-twitter" aria-hidden="true"></span>
      <span class="sr-only">Twitter</span>
    </a>
  

  
  <!--- Share on Facebook -->
    <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2F2019-07-01-pca-on-wine-data%2F"
      class="btn btn-social-icon btn-facebook" title="Share on Facebook">
      <span class="fa fa-fw fa-facebook" aria-hidden="true"></span>
      <span class="sr-only">Facebook</span>
    </a>
  

  
  <!--- Share on LinkedIn -->
    <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2F2019-07-01-pca-on-wine-data%2F"
      class="btn btn-social-icon btn-linkedin" title="Share on LinkedIn">
      <span class="fa fa-fw fa-linkedin" aria-hidden="true"></span>
      <span class="sr-only">LinkedIn</span>
    </a>
  

</section>



      

      <ul class="pager blog-pager">
        
        <li class="previous">
          <a href="/2019-05-01-rat-movement-analysis/" data-toggle="tooltip" data-placement="top" title="Rat Movement Analysis">&larr; Previous Post</a>
        </li>
        
        
        <li class="next">
          <a href="/2019-07-01-classification-on-digits-data/" data-toggle="tooltip" data-placement="top" title="Digit recognition using MNIST Database">Next Post &rarr;</a>
        </li>
        
      </ul>

      
        <div class="disqus-comments">
          
        </div>
          
        <div class="staticman-comments">
          

        </div>
        <div class="justcomments-comments">
          
        </div>
      
    </div>
  </div>
</div>


    <footer>
  <div class="container beautiful-jekyll-footer">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center footer-links"><li><a href="/feed.xml" title="RSS"><span class="fa-stack fa-lg" aria-hidden="true">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
                </span>
                <span class="sr-only">RSS</span>
              </a>
            </li><li><a href="mailto:sgok7@yahoo.com" title="Email me"><span class="fa-stack fa-lg" aria-hidden="true">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-envelope fa-stack-1x fa-inverse"></i>
                </span>
                <span class="sr-only">Email me</span>
              </a>
            </li><li><a href="https://github.com/goksinan" title="GitHub"><span class="fa-stack fa-lg" aria-hidden="true">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                </span>
                <span class="sr-only">GitHub</span>
              </a>
            </li><li><a href="https://twitter.com/snn_gk" title="Twitter"><span class="fa-stack fa-lg" aria-hidden="true">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                </span>
                <span class="sr-only">Twitter</span>
              </a>
            </li><li><a href="https://linkedin.com/in/sinangok" title="LinkedIn"><span class="fa-stack fa-lg" aria-hidden="true">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                </span>
                <span class="sr-only">LinkedIn</span>
              </a>
            </li></ul>
      <p class="copyright text-muted">
      Sinan Gok
      &nbsp;&bull;&nbsp;
      2019

      
      &nbsp;&bull;&nbsp;
      <a href="http://localhost:4000/">Home</a>
      

      
      </p>
          <!-- Please don't remove this, keep my open source work credited :) -->
    <p class="theme-by text-muted">
      Theme by
      <a href="https://deanattali.com/beautiful-jekyll/">beautiful-jekyll</a>
    </p>
      </div>
    </div>
  </div>
</footer>

  
    


  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
      <script>
      	if (typeof jQuery == 'undefined') {
          document.write('<script src="/js/jquery-1.11.2.min.js"></scr' + 'ipt>');
      	}
      </script>
    
  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
      <script src="/js/bootstrap.min.js"></script>
    
  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
      <script src="/js/main.js"></script>
    
  






  
  </body>
</html>
